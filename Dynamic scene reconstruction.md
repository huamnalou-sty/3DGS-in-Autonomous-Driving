**\[21]. ProDyG: Progressive Dynamic Scene Reconstruction via Gaussian Splatting from Monocular Videos**

* ğŸ§‘â€ğŸ“ **Author**ï¼šXiaoting Yin, Hao Shi, Kailun Yang, Jiajun Zhai, Shangwei Guo, Lin Wang, Kaiwei Wang
* ğŸ”— **Link**ï¼š\[[arXiv:2509.18566](https://arxiv.org/abs/2509.18566)]
* ğŸ¤” **Challenge**ï¼šReconstructing dynamic humans together with static scenes from monocular videos remains difficult, especially under fast motion, where RGB frames suffer from motion blur.
* ğŸ“– **Introduction**ï¼šThis paper presents a novel event-guided human-scene reconstruction framework that jointly models human and scene from a single monocular event camera via 3D Gaussian Splatting. By assigning a learnable semantic attribute to each Gaussian and introducing an event-driven supervision loss, our method removes the need for external human masks and robustly mitigates motion blur. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers state-of-the-art human-scene reconstruction, with notable gains over strong baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.

**\[20]. ProDyG: Progressive Dynamic Scene Reconstruction via Gaussian Splatting from Monocular Videos**

* ğŸ§‘â€ğŸ“ **Author**ï¼šShi Chen, Erik Sandstrom, Sandro Lombardi, Siyuan Li, Martin R. Oswald
* ğŸ”— **Link**ï¼š\[[arXiv:2509.17864](https://arxiv.org/abs/2509.17864)]
* ğŸ¤” **Challenge**ï¼šAchieving truly practical dynamic 3D reconstruction requires online operation, global pose and map consistency, detailed appearance modeling, and the flexibility to handle both RGB and RGB-D inputs. However, existing SLAM methods typically merely remove the dynamic parts or require RGB-D input, while offline methods are not scalable to long video sequences, and current transformer-based feedforward methods lack global consistency and appearance details.
* ğŸ“– **Introduction**ï¼šProDyG introduces a progressive Gaussian Splatting framework for monocular video-based dynamic scene reconstruction, enabling online processing with global consistency. It progressively builds and refines the scene representation, supporting both RGB and RGB-D inputs, and achieves high-fidelity novel view synthesis while maintaining real-time performance in long sequences.

**\[19]. BARD-GS: Blur-Aware Reconstruction of Dynamic Scenes via Gaussian Splatting (CVPR 2025)**

* ğŸ§‘â€ğŸ“ **Author**ï¼šYiren Lu, Yunlai Zhou, Disheng Liu, Tuo Liang, Yu Yin
* ğŸ”— **Link**ï¼š\[[arXiv:2503.15835](https://arxiv.org/abs/2503.15835)]
* ğŸ¤” **Challenge**ï¼šMotion blur in real-world dynamic scenes captured by handheld monocular cameras poses significant challenges for accurate 3D reconstruction, particularly in distinguishing camera motion blur from object motion blur.
* ğŸ“– **Introduction**ï¼šBARD-GS proposes a blur-aware Gaussian Splatting method that explicitly models and deblurs dynamic scenes by integrating motion estimation and adaptive Gaussian deformation. It outperforms prior methods in handling blurred inputs, achieving superior reconstruction quality and rendering fidelity on dynamic benchmarks.
* 
**\[18]. SurgicalGS: Dynamic 3D Gaussian Splatting for Accurate Robotic-Assisted Surgical Scene Reconstruction (MICCAI 2025)**

* ğŸ§‘â€ğŸ“ **Author**ï¼šJialei Chen, Xin Zhang, Mobarakol Islam, Francisco Vasconcelos, Danail Stoyanov, Daniel S. Elson, Baoru Huang
* ğŸ”— **Link**ï¼š\[[arXiv:2410.09292](https://arxiv.org/abs/2410.09292)]
* ğŸ¤” **Challenge**ï¼šAccurate 3D reconstruction of dynamic surgical scenes from endoscopic video is essential for robotic-assisted surgery, but recent 3D Gaussian Splatting methods suffer from depth compression due to inverse depth loss functions, leading to inaccuracies in deformable tissues.
* ğŸ“– **Introduction**ï¼šSurgicalGS enhances dynamic Gaussian Splatting with a novel depth-aware loss and deformation modeling tailored for surgical environments, enabling precise reconstruction of deformable scenes. It achieves higher geometric accuracy and faster rendering compared to baselines, supporting real-time robotic applications.

**\[17]. Motion-Blender Gaussian Splatting for Dynamic Scene Reconstruction**

* ğŸ§‘â€ğŸ“ **Author**ï¼šXinyu Zhang, Haonan Chang, Yuhan Liu, Abdeslam Boularias
* ğŸ”— **Link**ï¼š\[[arXiv:2503.09040](https://arxiv.org/abs/2503.09040)]
* ğŸ¤” **Challenge**ï¼šExisting dynamic Gaussian Splatting methods struggle with explicit motion representation, leading to inefficiencies in modeling complex non-rigid deformations in robot vision applications.
* ğŸ“– **Introduction**ï¼šThis work introduces Motion-Blender Gaussian Splatting (MBGS), a framework that uses motion graphs as an explicit and sparse motion representation, propagating link motions to individual Gaussians via dual quaternion skinning with learnable weights. Jointly optimized from videos, it enables high-fidelity dynamic reconstruction with differentiable rendering.
* 
**\[16]. DGNS: Deformable Gaussian Splatting and Dynamic Neural Surface for Monocular Dynamic 3D Reconstruction(ACM MM 2025)**

* ğŸ§‘â€ğŸ“ **Author**ï¼šXuesong Li, Jinguang Tong, Jie Hong, Vivien Rolland, Lars Petersson
* ğŸ”— **Link**ï¼š\[[arXiv:2412.03910](https://arxiv.org/abs/2412.03910)]
* ğŸ¤” **Challenge**ï¼šMonocular dynamic 3D reconstruction faces challenges in novel-view synthesis and geometry fidelity due to the lack of explicit surface constraints in pure Gaussian representations.
* ğŸ“– **Introduction**ï¼šDGNS combines Deformable Gaussian Splatting with Dynamic Neural Surfaces in a hybrid framework, incorporating depth and normal supervision for improved 3D consistency. It excels in dynamic scene reconstruction, outperforming baselines in PSNR and geometric metrics on standard datasets.
* 
**\[15]. Periodic Vibration Gaussian: Dynamic Urban Scene Reconstruction and Real-time Rendering(CVPR 2024)**

* ğŸ§‘â€ğŸ“ **Author**ï¼šYurui Chen, Chun Gu, Junzhe Jiang, Xiatian Zhu, Li Zhang
* ğŸ”— **Link**ï¼š\[[arXiv:2311.18561](https://arxiv.org/abs/2311.18561)]
* ğŸ¤” **Challenge**ï¼šThe modeling challenge for dynamic, large-scale urban scenes lies in their highly complex geometric structures and spatially and temporally unconstrained dynamics. Prior approaches typically employ high-level architectural priors that separate static and dynamic elements, resulting in suboptimal capture of their synergistic interactions.
* ğŸ“– **Introduction**ï¼šThis paper introduces Periodic Vibration Gaussian (PVG), which builds upon 3DGS by incorporating time-based dynamics driven by periodic vibrations. This innovation enables PVG to elegantly and uniformly represent the characteristics of various objects and elements within dynamic urban scenes.

**\[14]. Speedy-Splat: Fast 3D Gaussian Splatting with Sparse Pixels and Sparse Primitives**

* ğŸ§‘â€ğŸ“ **Author**ï¼šAlex Hanson, Allen Tu, Geng Lin, Vasu Singla, Matthias Zwicker, Tom Goldstein
* ğŸ”— **Link**ï¼š\[[arXiv:2412.00578](https://arxiv.org/abs/2412.00578)]
* ğŸ¤” **Challenge**ï¼šAlthough recent work on compressing 3D-GS models has achieved some acceleration by reducing the number of parameters, few methods directly target rendering speed.
* ğŸ“– **Introduction**ï¼šSpeedy-Splat optimizes the rendering pipeline to precisely locate Gaussians within scenes, boosting rendering speed without compromising visual fidelity. Furthermore, it introduces a novel pruning technique integrated into the training pipeline, significantly reducing model size and training time while further accelerating rendering performance.

**\[13]. SplatFlow: Self-Supervised Dynamic Gaussian Splatting in Neural Motion Flow Field for Autonomous Driving(CVPR 2025)**

* ğŸ§‘â€ğŸ“ **Author**ï¼šSu Sun, Cheng Zhao, Zhuoyang Sun, Yingjie Victor Chen, Mei Chen
* ğŸ”— **Link**ï¼š\[[arXiv:2411.15482](https://arxiv.org/abs/2411.15482)]
* ğŸ¤” **Challenge**ï¼šMost existing dynamic Gaussian dispersion methods for complex urban scenes rely on costly, manually labeled, accurate object-level supervision, which limits their scalability in practical applications.
* ğŸ“– **Introduction**ï¼šThis paper introduces SplatFlow, a self-supervised dynamic Gaussian splatting technique within Neural Motion Flow Fields (NMFF) that learns 4D spatiotemporal representations without tracking 3D bounding boxes. This enables accurate dynamic scene reconstruction and novel view synthesis of RGB/depth/flow.

**\[12]. Das3r: Dynamics-aware gaussian splatting for static scene reconstruction**

* ğŸ§‘â€ğŸ“ **Author**ï¼šKai Xu, Tze Ho Elden Tse, Jizong Peng, Angela Yao
* ğŸ”— **Link**ï¼š\[[arXiv:2412.19584](https://arxiv.org/abs/2412.19584)]
* ğŸ¤” **Challenge**ï¼šPrevious 3DGS dynamic reconstruction methods have encountered difficulties when applied to broader everyday video content involving continuous and numerous dynamic components alongside complex camera movements.
* ğŸ“– **Introduction**ï¼šThis paper proposes DAS3R for scene segmentation and static background reconstruction from everyday videos. By integrating pre-trained motion masks, modeling static scenes as Gaussian fragments, and performing dynamics-aware optimization, it achieves more accurate background reconstruction results than previous work.

**\[11]. DeSiRe-GS: 4D Street Gaussians for Static-Dynamic Decomposition and Surface Reconstruction for Urban Driving Scenes(CVPR 2025)**

* ğŸ§‘â€ğŸ“ **Author**ï¼šChensheng Peng, Chengwei Zhang, Yixiao Wang, Chenfeng Xu, Yichen Xie, Wenzhao Zheng, Kurt Keutzer, Masayoshi Tomizuka, Wei Zhan
* ğŸ”— **Link**ï¼š\[[arXiv:2411.11921](https://arxiv.org/abs/2411.11921)]
* ğŸ¤” **Challenge**ï¼šDue to the unbounded nature of outdoor environments, 3DGS faces challenges in reconstruction accuracy and efficiency during dynamic driving scene reconstruction.
* ğŸ“– **Introduction**ï¼šThis paper proposes DeSiRe-GS, a two-stage optimization pipeline based on dynamic street Gaussians. In the first stage, a two-dimensional motion mask is extracted, as the observed three-dimensional Gaussian distribution inherently can only reconstruct static regions in dynamic environments. These extracted 2D motion priors are then differentially mapped into Gaussian space using the efficient formula of the second-stage dynamic Gaussian. Combined with introduced geometric regularization, this approach resolves overfitting issues caused by data sparsity in autonomous driving while reconstructing physically plausible Gaussians.

**\[10]. Mini-Splatting: Representing Scenes with a Constrained Number of Gaussians**

* ğŸ§‘â€ğŸ“ **Author**ï¼šGuangchi Fang, Bing Wang
* ğŸ”— **Link**ï¼š\[[arXiv:2403.14166v3](https://arxiv.org/abs/2403.14166v3)]
* ğŸ¤” **Challenge**ï¼šUsing a finite number of Gaussian representations to effectively model scenes
* ğŸ“– **Introduction**ï¼šThis paper introduces strategies for densification including blur split and depth reinitialization, and simplification through intersection preserving and sampling. These techniques reorganize the spatial positions of the Gaussians, resulting in significant improvements across various datasets and benchmarks in terms of rendering quality, resource consumption, and storage compression.

**\[9]. Street Gaussians: Modeling Dynamic Urban Scenes with Gaussian Splatting(ECCV 2024)**

* ğŸ§‘â€ğŸ“ **Author**ï¼šYunzhi Yan, Haotong Lin, Chenxu Zhou, Weijie Wang, Haiyang Sun, Kun Zhan, Xianpeng Lang, Xiaowei Zhou, Sida Peng
* ğŸ”— **Link**ï¼š\[[arXiv:2401.01339](https://arxiv.org/abs/2401.01339)]
* ğŸ¤” Challengeï¼š3DGS exhibits slow training and rendering speeds when modeling dynamic urban streets in autonomous driving scenarios.
* ğŸ“– **Introduction**ï¼šThe dynamic urban scenes of Street Gaussians are represented as a set of point clouds equipped with semantic logic and 3D Gaussians, each of which is associated with either a foreground vehicle or the background. To model the dynamics of foreground objects, each object point cloud is optimized using an optimizable tracking pose and a 4D spherical harmonic model for dynamic appearance. Street Gaussians enable scene editing operations and rendering at 135 FPS (1066 x 1600 resolution) within half an hour after training.

**\[8]. 4D Gaussian Splatting for Real-Time Dynamic Scene Rendering(CVPR 2024)**

* ğŸ§‘â€ğŸ“ **Author**ï¼šGuanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, Xinggang Wang
* ğŸ”— **Link**ï¼š\[[arXiv:2310.08528](https://arxiv.org/abs/2310.08528)]
* ğŸ¤” **Challenge**ï¼š3DGS accurately models complex motion, which is often difficult to achieve efficiently.
* ğŸ“– **Introduction**ï¼šThis paper proposes 4D Gaussian Splatting (4D-GS) as a holistic representation of dynamic scenes, rather than applying 3D-GS to each individual frame. In 4D-GS, a novel explicit representation is proposed that simultaneously incorporates 3D Gaussian and 4D neurovoxels.

**\[7]. AutoSplat: Constrained Gaussian Splatting for Autonomous Driving Scene Reconstruction**

* ğŸ§‘â€ğŸ“ **Author**ï¼šMustafa Khan, Hamidreza Fazlali, Dhruv Sharma, Tongtong Cao, Dongfeng Bai, Yuan Ren, Bingbing Liu
* ğŸ”— **Link**ï¼š\[[arXiv:2407.02598](https://arxiv.org/abs/2407.02598)]
* ğŸ¤” **Challenge**ï¼š3DGS faces challenges in simulating driving scenarios due to complex backgrounds, dynamic objects, and sparse views.
* ğŸ“– **Introduction**ï¼šThis paper proposes AutoSplat to achieve highly realistic reconstruction of autonomous driving scenes. It decomposes the background and imposes geometric constraints on its road and sky regions to enable multi-view consistent rasterization. It uses 3D templates to initialize foreground Gaussians and combines reflection Gaussian consistency constraints to reconstruct invisible parts from symmetric visible views. It captures the dynamic visual features of foreground objects by estimating time-dependent residual spherical harmonics. AutoSplat outperforms state-of-the-art methods in scene reconstruction and novel view synthesis across various driving scenarios.

**\[6]. VDG: Vision-Only Dynamic Gaussian for Driving Simulation**

* ğŸ§‘â€ğŸ“ **Author**ï¼šHao Li, Jingfeng Li, Dingwen Zhang, Chenming Wu, Jieqi Shi, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang, Junwei Han
* ğŸ”— **Link**ï¼š\[[arXiv:2406.18198](https://arxiv.org/abs/2406.18198)]
* ğŸ¤” **Challenge**ï¼šPrevious works heavily relied on precomputed pose and motion structure (SfM) algorithms or expensive sensors with Gaussian initialization.
* ğŸ“– **Introduction**ï¼šThis paper promotes pose and depth initialization as well as static-dynamic decomposition by integrating self-supervised VO into our pose-free dynamic Gaussian method (VDG) for the first time. In addition, compared with pose-free dynamic view synthesis methods, VDG can process only RGB image inputs and construct dynamic scenes at a faster speed and with a larger scene.

**\[5]. S3Gaussian: Self-Supervised Street Gaussians for Autonomous Driving(CVPR 2024)**

* ğŸ§‘â€ğŸ“ **Author**ï¼šNan Huang, Xiaobao Wei, Wenzhao Zheng, Pengju An, Ming Lu, Wei Zhan, Masayoshi Tomizuka, Kurt Keutzer, Shanghang Zhang
* ğŸ”— **Link**ï¼š\[[arXiv:2405.20323](https://arxiv.org/abs/2405.20323)]
* ğŸ¤” **Challenge**ï¼šMost previous street 3DGS methods require tracked 3D vehicle bounding boxes to decompose the static and dynamic elements for effective reconstruction, limiting their applications for in-the-wild scenarios.
* ğŸ“– **Introduction**ï¼šThis paper proposes a self-supervised street Gaussian (S3Gaussian) method that decomposes dynamic and static elements from 4D consistency. S3Gaussian demonstrates the ability to decompose static and dynamic scenes and achieves optimal performance without using 3D annotations.

**\[4]. DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic Autonomous Driving Scenes(CVPR 2024)**

* ğŸ§‘â€ğŸ“ **Author**ï¼šXiaoyu Zhou, Zhiwei Lin, Xiaojun Shan, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang
* ğŸ”— **Link**ï¼š\[[arXiv:2312.07920](https://arxiv.org/abs/2312.07920)]
* ğŸ¤” **Challenge**ï¼šPrevious works focused on individual dynamic objects and were unable to handle complex driving scenarios involving static-dynamic composite areas and multiple high-speed moving objects.
* ğŸ“– **Introduction**ï¼šDrivingGaussian uses composite dynamic Gaussian maps to process multiple moving objects, independently reconstructing each object and restoring its accurate position and occlusion relationships in the scene. DrivingGaussian outperforms existing methods in dynamic driving scene reconstruction and can achieve realistic surround synthesis with high fidelity and multi-camera consistency.

**\[3]. HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting**

* ğŸ§‘â€ğŸ“ **Author**ï¼šHongyu Zhou, Jiahao Shao, Lu Xu, Dongfeng Bai, Weichao Qiu, Bingbing Liu, Yue Wang, Andreas Geiger, Yiyi Liao
* ğŸ”— **Link**ï¼š\[[arXiv:2403.12722](https://arxiv.org/abs/2403.12722)]
* ğŸ¤” **Challenge**ï¼šPrevious works have typically focused on specific aspects of understanding urban scenes based on RGB images, requiring additional inputs such as LiDAR scans or manually annotated 3D bounding boxes.
* ğŸ“– **Introduction**ï¼šThis paper introduces a new pipeline for understanding urban scenes using 3D Gaussian splatting, which can render new viewpoints in real time, generate high-precision 2D and 3D semantic information, and reconstruct dynamic scenes, even in scenes with high noise in 3D bounding box detection.

**\[2]. Motion-aware 3D Gaussian Splatting  for Efficient Dynamic Scene Reconstruction**

* ğŸ§‘â€ğŸ“ **Author**ï¼šZhiyang Guo, Wengang Zhou, Li Li, Min Wang, Houqiang Li
* ğŸ”— **Link**ï¼š\[[arXiv:2403.11447](https://arxiv.org/abs/2403.11447)]
* ğŸ¤” **Challenge**ï¼šPrevious works have primarily focused on extending static 3DGS to time-varying representations, neglecting the rich motion information carried by two-dimensional observations. This approach suffers from performance degradation and model redundancy issues.
* ğŸ“– **Introduction**ï¼šA novel motion-aware enhancement framework for dynamic scene reconstruction is proposed, which extracts useful motion cues from optical flow to improve different paradigms of dynamic 3DGS. Compared with the baselines, our method shows significant superiority in both rendering quality and efficiency.

**\[1]. Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis**

* ğŸ§‘â€ğŸ“ **Author**ï¼šJonathon Luiten, Georgios Kopanas, Bastian Leibe, Deva Ramanan
* ğŸ”— **Link**ï¼š\[[arXiv:2308.09713](https://arxiv.org/abs/2308.09713)]
* ğŸ¤” **Challenge**ï¼šExtend 3DGS from modeling static scenes only to dynamic scenes.
* ğŸ“– **Introduction**ï¼šDynamic 3D Gaussians solve the task of dynamic scene view synthesis and six degrees of freedom (6-DOF) tracking of all dense scene elements, while supporting a wide range of downstream applications, including first-person view synthesis, dynamic scene synthesis, and 4D video editing.
