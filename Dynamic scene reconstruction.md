**\[1]. Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis**

* ğŸ§‘â€ğŸ“ **Author**ï¼šJonathon Luiten, Georgios Kopanas, Bastian Leibe, Deva Ramanan
* ğŸ”— **Link**ï¼š\[[arXiv:2308.09713](https://arxiv.org/abs/2308.09713)]
* ğŸ“– **Introduction**ï¼šDynamic 3D Gaussians solve the task of dynamic scene view synthesis and six degrees of freedom (6-DOF) tracking of all dense scene elements, while supporting a wide range of downstream applications, including first-person view synthesis, dynamic scene synthesis, and 4D video editing.



**\[2]. Motion-aware 3D Gaussian Splatting  for Efficient Dynamic Scene Reconstruction**

* ğŸ§‘â€ğŸ“ **Author**ï¼šZhiyang Guo, Wengang Zhou, Li Li, Min Wang, Houqiang Li
* ğŸ”— **Link**ï¼š\[[arXiv:2403.11447](https://arxiv.org/abs/2403.11447)]
* ğŸ“– **Introduction**ï¼šA novel motion-aware enhancement framework for dynamic scene reconstruction is proposed, which extracts useful motion cues from optical flow to improve different paradigms of dynamic 3DGS. Compared with the baselines, our method shows significant superiority in both rendering quality and efficiency.



**\[3]. HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting**

* ğŸ§‘â€ğŸ“ **Author**ï¼šHongyu Zhou, Jiahao Shao, Lu Xu, Dongfeng Bai, Weichao Qiu, Bingbing Liu, Yue Wang, Andreas Geiger, Yiyi Liao
* ğŸ”— **Link**ï¼š\[[arXiv:2403.12722](https://arxiv.org/abs/2403.12722)]
* ğŸ“– **Introduction**ï¼šThis paper introduces a new pipeline for understanding urban scenes using 3D Gaussian splatting, which can render new viewpoints in real time, generate high-precision 2D and 3D semantic information, and reconstruct dynamic scenes, even in scenes with high noise in 3D bounding box detection.



**\[4]. DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic Autonomous Driving Scenes**

* ğŸ§‘â€ğŸ“ **Author**ï¼šXiaoyu Zhou, Zhiwei Lin, Xiaojun Shan, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang
* ğŸ”— **Link**ï¼š\[arXiv:2312.07920]
* ğŸ“– **Introduction**ï¼šDrivingGaussian uses composite dynamic Gaussian maps to process multiple moving objects, independently reconstructing each object and restoring its accurate position and occlusion relationships in the scene. DrivingGaussian outperforms existing methods in dynamic driving scene reconstruction and can achieve realistic surround synthesis with high fidelity and multi-camera consistency.



**\[5]. S3Gaussian: Self-Supervised Street Gaussians for Autonomous Driving**

* ğŸ§‘â€ğŸ“ **Author**ï¼šNan Huang, Xiaobao Wei, Wenzhao Zheng, Pengju An, Ming Lu, Wei Zhan, Masayoshi Tomizuka, Kurt Keutzer, Shanghang Zhang
* ğŸ”— **Link**ï¼š\[[arXiv:2405.20323](https://arxiv.org/abs/2405.20323)]
* ğŸ“– **Introduction**ï¼šTo promote efficient 3D scene reconstruction without expensive annotations, this paper proposes a self-supervised street Gaussian (S3Gaussian) method that decomposes dynamic and static elements from 4D consistency. S3Gaussian demonstrates the ability to decompose static and dynamic scenes and achieves optimal performance without using 3D annotations.



**\[6]. VDG: Vision-Only Dynamic Gaussian for Driving Simulation**

* ğŸ§‘â€ğŸ“ **Author**ï¼šHao Li, Jingfeng Li, Dingwen Zhang, Chenming Wu, Jieqi Shi, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang, Junwei Han
* ğŸ”— **Link**ï¼š\[[arXiv:2406.18198](https://arxiv.org/abs/2406.18198)]
* ğŸ“– **Introduction**ï¼šThis paper promotes pose and depth initialization as well as static-dynamic decomposition by integrating self-supervised VO into our pose-free dynamic Gaussian method (VDG) for the first time. In addition, compared with pose-free dynamic view synthesis methods, VDG can process only RGB image inputs and construct dynamic scenes at a faster speed and with a larger scene.



**\[7]. AutoSplat: Constrained Gaussian Splatting for Autonomous Driving Scene Reconstruction**

* ğŸ§‘â€ğŸ“ **Author**ï¼šMustafa Khan, Hamidreza Fazlali, Dhruv Sharma, Tongtong Cao, Dongfeng Bai, Yuan Ren, Bingbing Liu
* ğŸ”— **Link**ï¼š\[[arXiv:2407.02598](https://arxiv.org/abs/2407.02598)]
* ğŸ“– **Introduction**ï¼šThis paper proposes AutoSplat to achieve highly realistic reconstruction of autonomous driving scenes. It decomposes the background and imposes geometric constraints on its road and sky regions to enable multi-view consistent rasterization. It uses 3D templates to initialize foreground Gaussians and combines reflection Gaussian consistency constraints to reconstruct invisible parts from symmetric visible views. It captures the dynamic visual features of foreground objects by estimating time-dependent residual spherical harmonics. AutoSplat outperforms state-of-the-art methods in scene reconstruction and novel view synthesis across various driving scenarios.



**\[8]. 4D Gaussian Splatting for Real-Time Dynamic Scene Rendering**

* ğŸ§‘â€ğŸ“ **Author**ï¼šGuanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, Xinggang Wang
* ğŸ”— **Link**ï¼š\[[arXiv:2310.08528](https://arxiv.org/abs/2310.08528)]
* ğŸ“– **Introduction**ï¼šThis paper proposes 4D Gaussian Splatting (4D-GS) as a holistic representation of dynamic scenes, rather than applying 3D-GS to each individual frame. In 4D-GS, a novel explicit representation is proposed that simultaneously incorporates 3D Gaussian and 4D neurovoxels.



**\[9]. Street Gaussians: Modeling Dynamic Urban Scenes with Gaussian Splatting**

* ğŸ§‘â€ğŸ“ **Author**ï¼šYunzhi Yan, Haotong Lin, Chenxu Zhou, Weijie Wang, Haiyang Sun, Kun Zhan, Xianpeng Lang, Xiaowei Zhou, Sida Peng
* ğŸ”— **Link**ï¼š\[[arXiv:2401.01339](https://arxiv.org/abs/2401.01339)]
* ğŸ“– **Introduction**ï¼šThe dynamic urban scenes of Street Gaussians are represented as a set of point clouds equipped with semantic logic and 3D Gaussians, each of which is associated with either a foreground vehicle or the background. To model the dynamics of foreground objects, each object point cloud is optimized using an optimizable tracking pose and a 4D spherical harmonic model for dynamic appearance. Street Gaussians enable scene editing operations and rendering at 135 FPS (1066 x 1600 resolution) within half an hour after training.



**\[10].** 

* ğŸ§‘â€ğŸ“ **Author**ï¼š
* ğŸ”— **Link**ï¼š
* ğŸ“– **Introduction**ï¼š
